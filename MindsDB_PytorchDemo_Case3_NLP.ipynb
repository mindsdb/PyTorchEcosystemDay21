{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "\n",
    "### Natasha Seelam (natasha@mindsdb.com), Patricio Cerda-Mardini, Cosmo Jenytin, George Hosu, Jorge Torres\n",
    "### 2021.04.21\n",
    "\n",
    "The following notebook is intended for users to experience MindsDB, and how to build a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Datasource is not available by default. If you wish to use it, please install mindsdb_native[extra_data_sources]\n",
      "Athena Datasource is not available by default. If you wish to use it, please install mindsdb_native[extra_data_sources]\n",
      "Google Cloud Storage Datasource is not available by default. If you wish to use it, please install mindsdb_native[extra_data_sources]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import mindsdb_native as mdb\n",
    "\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f51c9a325f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup torch seed for reproducibility\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Load the dataset\n",
    "\n",
    "In the following, we will use the \"Stanford IMDB\" dataset, a dataset that is commonly used to benchmark text sentiment and classification for natural language processing (NLP).\n",
    "\n",
    "The leaderboard results suggest 95% accuracy is generally the state-of-the art, as indicated [here](https://paperswithcode.com/sota/text-classification-on-imdb). We're going to build a predictor for text classification with just a few lines of code.\n",
    "\n",
    "First, we load the stanford IMDB dataset. Given the way the data was read, we scramble them in arbitrary order to ensure we have a roughly homogenous distribution of positive (1) and negative (0) examples.\n",
    "\n",
    "The dataset has two columns, \"Comments\", which are the text reviews of the movie, and \"Polarity\", which indicate the sentiment as positive or negative. Our task will be to convert the comments into a featurized form, and train a model to predict the polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20308</th>\n",
       "      <td>I had high hopes going to see this, as I alway...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37706</th>\n",
       "      <td>This is an enjoyable movie. Its very realistic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>This should be re-titled \"The Curious Case Of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42143</th>\n",
       "      <td>Lucasarts have pulled yet another beauty out o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23202</th>\n",
       "      <td>Blackwater Valley Exorcism is a movie about a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Comments  Polarity\n",
       "20308  I had high hopes going to see this, as I alway...         0\n",
       "37706  This is an enjoyable movie. Its very realistic...         1\n",
       "6041   This should be re-titled \"The Curious Case Of ...         0\n",
       "42143  Lucasarts have pulled yet another beauty out o...         1\n",
       "23202  Blackwater Valley Exorcism is a movie about a ...         0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"datasets/\"\n",
    "dataset = \"stanford_movie_review\"\n",
    "filename = os.path.join(data_dir, dataset, \"data.csv\")\n",
    "\n",
    "\n",
    "# Load the data, and scramble the order. The way the data has been \n",
    "data = pd.read_csv(filename).sample(frac=1, random_state=1234)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example of the model as such for a positive versus negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPositive (Label=1) Review:\n",
      "\u001b[0m This is an enjoyable movie. Its very realistic to the \"wonderful world of music\" I've been there and done that. It shows a human element in each character and the realism that nobody is perfect. These amateur musicians weren't all that bad players. Cleavon Little's character, Marshall Tucker, was played very well. Marshall was no saint himself. Here he was getting paid to do a job and he's giving these guys a hard time about everything in the van on the way up there. You don't bite the hands that feed you. I do find it hard to believe that a player with the jazz experience he has, claims he does not know any of the dixieland tunes. He has a tremendous sense of predicting chord changes to tunes he does not know. Not common, but not unheard of either. He delivers a true and harsh message at the end of the movie when he tells the clarinet player, \"its not a religion, devotion is not enough.\" On that level, he is correct, although I think the clarinet player could have handled the job. He was practicing his butt off and vocal accompaniment music is not that hard to read. Very enjoyable movie.\n",
      "\u001b[1m\n",
      "\n",
      "Negative (Label=0) Review: \n",
      "\u001b[0m I had high hopes going to see this, as I always enjoy Paul Bettany's performances. I thought he was very good as Darwin, and did his best considering the terrible material he had to work with.Darwin's book On the Origin of Species was one of the most ground-breaking, controversial and innovative publications ever, yet you'd never think it based on this tedious movie. It's like a two-hour episode of a soap opera in a Victorian setting. There is virtually nothing about Darwin's five-year voyage on The Beagle to the Galapagos Islands, for example, surely of supreme significance to the story, as it was from his investigations of the wildlife thereon that he began to form his theory of evolution.This is just one long, dreary, domestic drama, with Darwin portrayed as a slightly loopy eccentric, seeing visions of his dead daughter everywhere and being given the cold shoulder by his emotionally-constipated wife. Jennifer Connelly's portrayal of Emma Darwin is nothing short of awful and bears little relation to historical descriptions of the real Emma. There could have been an opportunity here to present the creationist interpretation of life on earth, from either Emma or from the local priest, as played by Jeremy Northam (a blink-and-you'd-miss-it part which is a complete waste of a talented actor) to act as a counterbalance to Darwin's views, but it wasn't taken up. The story focused too much on endless mawkish sentiment about Darwin's grief for his daughter Annie, and too much time was also wasted in Darwin wondering whether or not to write his book. Eventually I was so bored it was difficult to care. All in all, this was a bit like making a movie about Picasso and spending two hours concentrating on him having a fight with his girlfriend and not bothering to mention that he was an artist.\n"
     ]
    }
   ],
   "source": [
    "# Rating examples\n",
    "print(\"\\033[1m\" + \"Positive (Label=1) Review:\\n\" + \"\\033[0m\", data[data[\"Polarity\"] == 1][\"Comments\"].iloc[0])\n",
    "\n",
    "print(\"\\033[1m\" + \"\\n\\nNegative (Label=0) Review: \\n\" + \"\\033[0m\", data[data[\"Polarity\"] == 0][\"Comments\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our data into 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our training/testing fraction manually\n",
    "\n",
    "ptrain = 0.8\n",
    "Ntrain = int(data.shape[0] * ptrain)\n",
    "Ntest = int(data.shape[0] - Ntrain)\n",
    "\n",
    "train = data.iloc[:Ntrain, :]\n",
    "test = data.iloc[Ntrain:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. Train a model!\n",
    "\n",
    "We'll train a model on our training data with simply 2 lines of code!\n",
    "\n",
    "**NOTE** without a GPU, this will likely take a very long time to take. With a GPU, this is probably about 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/controllers/predictor.py:211 - Sample for analysis: True\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataExtractor\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataExtractor, execution time: 0.053 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataCleaner\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_cleaner/data_cleaner.py:29 - Dropped 276 duplicate rows.\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataCleaner, execution time: 0.073 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] TypeDeductor\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/type_deductor/type_deductor.py:311 - Analyzing a sample of 13169 from a total population of 39724, this is equivalent to 33.2% of your data.\n",
      "\u001b[0m\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pcerdam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/type_deductor/type_deductor.py:405 - Data distribution for column \"Comments\" of type \"Categorical\" and subtype \"Binary Category\"\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:81 - ----------\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:124 -  Binary Category: 13169\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:128 - ----------\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/type_deductor/type_deductor.py:405 - Data distribution for column \"Polarity\" of type \"Categorical\" and subtype \"Binary Category\"\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:81 - ----------\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:124 -  Binary Category: 13169\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/data_types/mindsdb_logger.py:128 - ----------\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] TypeDeductor, execution time: 102.144 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataAnalyzer\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:256 - Analyzing a sample of 13169 from a total population of 39724, this is equivalent to 33.2% of your data.\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:268 - Analyzing column: Comments !\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:326 - Your column has 1 values missing\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:326 - You may want to check if you see something suspicious on the right-hand-side graph. This doesn't necessarily mean there's an issue with your data, it just indicates a higher than usual probability there might be some issue.\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:346 - Finished analyzing column: Comments !\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:268 - Analyzing column: Polarity !\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_analyzer/data_analyzer.py:346 - Finished analyzing column: Polarity !\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataAnalyzer, execution time: 0.827 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataCleaner\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataCleaner, execution time: 0.041 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataSplitter\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/data_splitter/data_splitter.py:109 - We have split the input data into:\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataSplitter, execution time: 1.780 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataTransformer\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataTransformer, execution time: 0.063 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] ModelInterface\n",
      "\u001b[0m\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/MindsDB/lightwood/lightwood/encoders/text/helpers/pretrained_helpers.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MindsDB/nenv/lib/python3.8/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/MindsDB/nenv/lib/python3.8/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "INFO:root:A single GBM itteration takes 0.1 seconds\n",
      "INFO:root:Training GBM (<module 'optuna.integration.lightgbm' from '/MindsDB/nenv/lib/python3.8/site-packages/optuna/integration/lightgbm.py'>) with 100 iterations\n",
      "feature_fraction, val_score: 0.208142: 100%|##########| 7/7 [03:06<00:00, 26.64s/it]\n",
      "num_leaves, val_score: 0.195868: 100%|##########| 20/20 [04:00<00:00, 12.04s/it]\n",
      "bagging, val_score: 0.195647: 100%|##########| 10/10 [00:49<00:00,  4.96s/it]\n",
      "feature_fraction_stage2, val_score: 0.195072: 100%|##########| 3/3 [00:16<00:00,  5.60s/it]\n",
      "regularization_factors, val_score: 0.193811: 100%|##########| 20/20 [02:02<00:00,  6.14s/it]\n",
      "min_data_in_leaf, val_score: 0.193259: 100%|##########| 5/5 [00:38<00:00,  7.76s/it]\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:462 - [LightGBMMixer] Training accuracy of: {'Polarity': {'function': 'accuracy_score', 'value': 0.9267618454372851}}\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Building network of shape: [768, 130, 3]\n",
      "INFO:lightwood.27704:Enabled dropout !\n",
      "INFO:lightwood.27704:Subtest test error: 0.2162102929183415 on subset 1, overall test error: 0.19976903237402438\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 7 with an accuracy of 92.47% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.21361725670950754 on subset 1, overall test error: 0.19777841120958328\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 14 with an accuracy of 92.5% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.2150987982749939 on subset 1, overall test error: 0.1989893063902855\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 21 with an accuracy of 92.62% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.215530840413911 on subset 1, overall test error: 0.19977576285600662\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 28 with an accuracy of 92.58% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.21829833941800253 on subset 1, overall test error: 0.20144718512892723\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 35 with an accuracy of 92.55% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.213785143835204 on subset 1, overall test error: 0.19846859574317932\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 42 with an accuracy of 92.57% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.21686737026487077 on subset 1, overall test error: 0.2020852141082287\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 49 with an accuracy of 92.58% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Finished fitting on 1 of 3 subset\n",
      "INFO:lightwood.27704:Subtest test error: 0.19576620842729295 on subset 2, overall test error: 0.1957533173263073\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 0 with an accuracy of 92.53% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.19515297881194524 on subset 2, overall test error: 0.1943822093307972\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 7 with an accuracy of 92.65% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.19577633695943014 on subset 2, overall test error: 0.19401276521384717\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 14 with an accuracy of 92.65% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.19637633647237504 on subset 2, overall test error: 0.19382140450179577\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 21 with an accuracy of 92.55% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.19925832535539353 on subset 2, overall test error: 0.19624022990465165\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 28 with an accuracy of 92.7% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.20310918986797333 on subset 2, overall test error: 0.19898000098764895\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 35 with an accuracy of 92.68% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Finished fitting on 2 of 3 subset\n",
      "INFO:lightwood.27704:Subtest test error: 0.17848003762108938 on subset 3, overall test error: 0.195221409201622\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 0 with an accuracy of 92.52% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.17928432566779 on subset 3, overall test error: 0.19580547586083413\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 7 with an accuracy of 92.63% on the testing dataset\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightwood.27704:Subtest test error: 0.17793701801981246 on subset 3, overall test error: 0.1937961731106043\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 14 with an accuracy of 92.65% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.17854809761047363 on subset 3, overall test error: 0.19597245901823043\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 21 with an accuracy of 92.55% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.18076452612876892 on subset 3, overall test error: 0.1984675109386444\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 28 with an accuracy of 92.7% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Subtest test error: 0.1907048523426056 on subset 3, overall test error: 0.20755879655480386\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[37mDEBUG:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:336 - We've reached training epoch nr 35 with an accuracy of 92.0% on the testing dataset\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Finished fitting on 3 of 3 subset\n",
      "INFO:lightwood.27704:Finished training model !\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/model_interface/lightwood_backend.py:462 - [NnMixer] Training accuracy of: {'Polarity': {'function': 'accuracy_score', 'value': 0.9265183895374485}}\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] ModelInterface, execution time: 1379.185 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] ModelAnalyzer\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[32mINFO:mindsdb-logger-15a7bd1a-a209-11eb-86d1-5320a42bf28d---no_report:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] ModelAnalyzer, execution time: 279.496 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a predictor instance\n",
    "model = mdb.Predictor(name=\"text_reviews\")\n",
    "\n",
    "# Train your model!\n",
    "model.learn(from_data=train, to_predict=\"Polarity\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict on the test set and see how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataExtractor\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataExtractor, execution time: 0.023 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] DataTransformer\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] DataTransformer, execution time: 0.015 seconds\n",
      "\u001b[0m\n",
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:51 - [START] ModelInterface\n",
      "\u001b[0m\n",
      "INFO:lightwood.27704:Computing device used: cuda\n",
      "INFO:lightwood.27704:Model predictions and decoding completed\n",
      "\u001b[32mINFO:mindsdb-logger-core-logger---:/MindsDB/mindsdb_native/mindsdb_native/libs/phases/base_module.py:56 - [END] ModelInterface, execution time: 61.053 seconds\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = result._data[\"Polarity\"]\n",
    "ytrue = test[\"Polarity\"].astype(str).tolist()\n",
    "\n",
    "acc = [ypred[j] == ytrue[j] for j in range(len(ypred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is = 0.928\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy is =\", sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. Studying what goes on under the hood\n",
    "\n",
    "The above model only required 2 lines of code to train.What is the underlying model?\n",
    "\n",
    "We can check this out by probing the model instance. By looking into the backend, we can see the types of encoders the mixer intakes.\n",
    "\n",
    "First, let's look into the configuration file used to construct the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_features': [{'name': 'Comments',\n",
       "   'type': 'text',\n",
       "   'grouped_by': False,\n",
       "   'encoder_attrs': {'original_type': None, 'secondary_type': None}}],\n",
       " 'output_features': [{'name': 'Polarity',\n",
       "   'type': 'categorical',\n",
       "   'grouped_by': False,\n",
       "   'weights': {'0': 5.047445992327882e-05, '1': 5.02209722780233e-05},\n",
       "   'encoder_attrs': {'predict_proba': True,\n",
       "    'original_type': None,\n",
       "    'secondary_type': None}}],\n",
       " 'data_source': {'cache_transformed_data': True},\n",
       " 'mixer': {'class': lightwood.mixers.nn.NnMixer,\n",
       "  'kwargs': {'callback_on_iter': None,\n",
       "   'eval_every_x_epochs': 15.5,\n",
       "   'stop_training_after_seconds': 16000.125}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transaction.model_backend.predictor.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the \"Polarity\" column is modeled categorically (as it's a binary label). The \"Comments\" column is modeled with \"text\". In `lightwood/api/data_sources.py`, you can see how each of these data attributes are mapped to a particular encoder.\n",
    "\n",
    "We can even directly query these encoders and access their models as such:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Polarity': <lightwood.encoders.categorical.autoencoder.CategoricalAutoEncoder at 0x7f4e5f49dee0>,\n",
       " 'Comments': <lightwood.encoders.text.pretrained.PretrainedLang at 0x7f4e5f49de20>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transaction.model_backend.predictor._mixer.encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each encoder has a \"prepared\" flag that indicatews whether it has been trained or not. You can choose to not prepare an encoder, but the default behavior is to do so during the learn() call in the predictor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared encoder [Comments]? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepared encoder [Comments]?\", \n",
    "      model.transaction.model_backend.predictor._mixer.encoders[\"Comments\"]._prepared\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can see the nature of the mixer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightwood.mixers.nn.NnMixer at 0x7f4e5f49d1c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transaction.model_backend.predictor._mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can look at the details of the mixer as such. As you can see, it is a class of pytorch model called \"net\":\n",
    "\n",
    "The mixer default here is nn.Mixer (specifically our DefaultNet). You can choose whatever type of model architecture you like to deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultNet(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=130, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=130, out_features=3, bias=True)\n",
       "  )\n",
       "  (_foward_net): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=130, bias=True)\n",
       "    (1): SELU()\n",
       "    (2): Dropout(p=0.0, inplace=False)\n",
       "    (3): Linear(in_features=130, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transaction.model_backend.predictor._mixer.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDB",
   "language": "python",
   "name": "mdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
